{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise 4 Answer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tWVt1pyF9nC",
        "colab_type": "text"
      },
      "source": [
        "# Please don't edit directly in this document. Create your own copy first.#\n",
        "\n",
        "# Exercise 4: CBC Edmonton News"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vd9PQ3iIGFxD",
        "colab_type": "text"
      },
      "source": [
        "#Extract\n",
        "First, let's run the cell below to import neccesary libraries. Although most of the commonly used Python libraries are pre-installed, new libraries can be installed as !pip install [package name] or !apt-get install [package name].\n",
        "\n",
        "##1. Libraries\n",
        "\n",
        "*   [requests](https://github.com/psf/requests): an elegant and simple HTTP library for Python.\n",
        "*   [Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/): Python library for pulling data out of HTML and XML files\n",
        "* [datetime](https://docs.python.org/2/library/datetime.html): classes for manipulating dates and times in both simple and complex ways"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu4L8UmSGJGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import datetime "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWPk2VIXIgCR",
        "colab_type": "text"
      },
      "source": [
        "Second, set the url of the website from which we'd like to extract data using the requests library that we imported in the first step. If the access was successful, you should see the output as <Response [200]>.\n",
        "\n",
        "## 2. Set the URL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0p4mhwLF1lJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the URL you want to scrape from\n",
        "url='https://www.cbc.ca/news/canada/edmonton'\n",
        "\n",
        "# Connect to the URL\n",
        "response = requests.get(url)\n",
        "response"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N95enT9InWN",
        "colab_type": "text"
      },
      "source": [
        "Third, parse the html with BeautifulSoup.\n",
        "\n",
        "## 3. Parse HTML file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb7lrD90IzR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Parse HTML and save to BeautifulSoup object\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "soup"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD-LAa9vJfSn",
        "colab_type": "text"
      },
      "source": [
        "Fourth, find an element with its attribute name. \n",
        "\n",
        "###Syntax: find_all(name, attrs)\n",
        "Find all elements following the same syntax rules.\n",
        "\n",
        "## 4. Extract title, url, and date."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZi1xEq-JiDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract \n",
        "results = soup.find_all('div', attrs={'featuredNews sclt-featuredTopStoriesDense'})\n",
        "\n",
        "# Create an empty list\n",
        "records = []\n",
        "\n",
        "# Create a loop to repeat this process and store the output in a list of tuples called records:\n",
        "for result in results:\n",
        "    title = result.find('h3').text\n",
        "    url = result.find('a')['href']\n",
        "    today = datetime.date.today()\n",
        "    records.append((title, url, today))\n",
        "\n",
        "records"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Cz1LBOO_59",
        "colab_type": "text"
      },
      "source": [
        "# Save in the dataframe\n",
        "Import the panda library. The file will be saved in the virtual machine, so in order to download a csv file to your local computer, you need to import *files* from google.colab. \n",
        "\n",
        "## 1. Library\n",
        "\n",
        "*   [pandas](https://pandas.pydata.org/): open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FySrobNQUL1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICQ5C4byUpQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a dataframe\n",
        "df = pd.DataFrame(records, columns=['title', 'url', 'date'])\n",
        "df"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}